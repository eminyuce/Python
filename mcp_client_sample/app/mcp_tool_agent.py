import os
import json
import requests
import openai
from dotenv import load_dotenv  # Optional: only needed if you use .env file

# ğŸ“¥ Load environment variables from a .env file (if using python-dotenv)
load_dotenv()

# ğŸ” Load API keys from environment variables (recommended for security)
openai.api_key = os.getenv("OPENAI_API_KEY")
MCP_API_KEY = os.getenv("MCP_API_KEY")  # Optional API key for your MCP server

# ğŸŒ MCP server endpoints
MCP_DISCOVERY_URL = os.getenv("MCP_DISCOVERY_URL")
MCP_TOOL_CALL_URL = os.getenv("MCP_TOOL_CALL_URL")

# ğŸ§¾ Common headers for requests
HEADERS = {
    "Content-Type": "application/json",
}
if MCP_API_KEY:
    HEADERS["X-API-KEY"] = MCP_API_KEY  # Optional header if your server requires authentication

# ğŸ“¡ Step 1: Fetch available tool functions from MCP server
def fetch_function_definitions_from_mcp():
    try:
        response = requests.get(MCP_DISCOVERY_URL, headers=HEADERS, timeout=5)
        response.raise_for_status()
    except requests.RequestException as e:
        raise RuntimeError(f"âŒ Error fetching MCP tools: {str(e)}")

    tools = response.json()
    print(f"ğŸ” Fetched Tools:{tools}")
    openai_functions = []
    for tool in tools:
        tool_name = tool.get("name", "unknown")
        for func in tool.get("functions", []):
            openai_functions.append({
                "name": func.get("name", ""),
                "description": func.get("description", ""),
                "parameters": func.get("parameters", {})
            })

    return openai_functions

# âš™ï¸ Step 2: Call a specific tool on the MCP server
def call_mcp_function(function_name, arguments):
    try:
        payload = {
            "tool": function_name,
            "parameters": arguments
        }

        response = requests.post(MCP_TOOL_CALL_URL, headers=HEADERS, json=payload, timeout=5)
        response.raise_for_status()
        return response.json()

    except requests.RequestException as e:
        return {"error": f"âŒ MCP Error: {str(e)}"}

# ğŸ§  Step 3: Full end-to-end roundtrip: user â†’ tool â†’ result â†’ LLM
def main():
    user_message = "What's the weather like in Istanbul?"

    # ğŸ” Discover tools from MCP
    print("ğŸ” Fetching tool definitions from MCP server...")
    function_definitions = fetch_function_definitions_from_mcp()

    # ğŸ§  Ask OpenAI model with function definitions
    print("ğŸ’¬ Sending user message to GPT with tool definitions...")
    response = openai.ChatCompletion.create(
        model="gpt-4-0613",  # Supports function calling
        messages=[{"role": "user", "content": user_message}],
        functions=function_definitions,
        function_call="auto",
    )

    message = response["choices"][0]["message"]

    # ğŸ” Check if GPT wants to call a tool
    if message.get("function_call"):
        func_name = message["function_call"]["name"]
        func_args = json.loads(message["function_call"]["arguments"])

        print(f"ğŸ“ GPT wants to call MCP function: {func_name} with args {func_args}")

        # ğŸ§ª Call the tool via MCP
        func_response = call_mcp_function(func_name, func_args)

        # ğŸ§  Feed result back into GPT for reasoning
        print("ğŸ” Sending tool response back to GPT for reasoning...")
        final_response = openai.ChatCompletion.create(
            model="gpt-4-0613",
            messages=[
                {"role": "user", "content": user_message},
                message,
                {
                    "role": "function",
                    "name": func_name,
                    "content": json.dumps(func_response)
                }
            ],
        )

        # âœ… Final answer from GPT
        print("\nâœ… Final answer from GPT:")
        print(final_response["choices"][0]["message"]["content"])

    else:
        # ğŸ¤– GPT answered without needing a tool
        print("\nğŸ¤– GPT Response:")
        print(message["content"])

# ğŸš€ Entry point
if __name__ == "__main__":
    main()
